# -*- coding: utf-8 -*-
"""pix2pix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb

##### Copyright 2019 The TensorFlow Authors.

Licensed under the Apache License, Version 2.0 (the "License");
"""

# TODO: JDD made it so this is toggleable for testing purposes.
use_training = False  # TODO: Was originally true

# If True, exports the model rather than train.
EXPORT_MODEL = False

# @title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import tensorflow as tf
import time
from .input_output.images.saving import generate_images
from .input_output.checkpoints import make_checkpoint, restore_checkpoint, save_checkpoint

from .generators.size64 import PATH, IMG_DIMEN, Generator, CHECKPOINT_DIR, BETA_1, ID
# from .generators.size256 import PATH, IMG_DIMEN, Generator, CHECKPOINT_DIR, BETA_1, ID
# from .generators.size512 import PATH, IMG_DIMEN, Generator, CHECKPOINT_DIR, BETA_1, ID

# from .generators.size256Full import PATH, IMG_DIMEN, Generator, CHECKPOINT_DIR
# from .generators.size256Full_v2 import PATH, IMG_DIMEN, Generator, CHECKPOINT_DIR
# from .generators.size256Full_v4 import PATH, IMG_DIMEN, Generator, CHECKPOINT_DIR

from .generators.loss import generator_loss
from .input_output.images.loading import load_image_train, load_from_img, load_image_test, get_random_img
from .vars import set_img_dimen
from .descriminator import Discriminator, discriminator_loss
from .input_output.datasets import load_datasets
from .input_output.summary import make_summary_writer, write_to_summary
from .input_output.export_model import export_model

import wandb

def main():
    wandb.init(project='pix2pix_prot2prot', id=ID, resume=True)

    """Each original image is of size `256 x 512` containing two `256 x 256` images:"""

    set_img_dimen(IMG_DIMEN)

    # sample_image = tf.io.read_file(str(PATH + 'train/0471202805568984.png'))
    # sample_image = tf.io.read_file(get_random_img(PATH))
    # sample_image = tf.io.decode_jpeg(sample_image)
    # print(sample_image.shape)

    """You need to separate real building facade images from the architecture
    label images—all of which will be of size `256 x 256`.

    Define a function that loads image files and outputs two image tensors:
    """

    """Plot a sample of the input (architecture label image) and real (building
    facade photo) images:"""

    # inp, re = load_from_img(str(PATH + 'train/0471202805568984.png'))
    inp, re = load_from_img(get_random_img(PATH))

    """As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you
    need to apply random jittering and mirroring to preprocess the training set.

    Define several functions that:

    1. Resize each `256 x 256` image to a larger height and width—`286 x 286`.
    2. Randomly crop it back to `256 x 256`.
    3. Randomly flip the image horizontally i.e. left to right (random mirroring).
    4. Normalize the images to the `[-1, 1]` range.
    """

    # The facade training set consist of 400 images
    BUFFER_SIZE = 25  # Was 400

    # The batch size of 1 produced better results for the U-Net in the original pix2pix experiment
    BATCH_SIZE = 1

    # Normalizing the images to [-1, 1]

    """## Build an input pipeline with `tf.data`"""
    train_dataset, test_dataset = load_datasets(PATH, BUFFER_SIZE, BATCH_SIZE)

    """## Build the generator

    The generator of your pix2pix cGAN is a _modified_
    [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder
    (downsampler) and decoder (upsampler). (You can find out more about it in the
    [Image segmentation](https://www.tensorflow.org/tutorials/images/segmentation)
    tutorial and on the [U-Net project
    website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).)

    - Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU
    - Each block in the decoder is: Transposed convolution -> Batch normalization ->
    Dropout (applied to the first 3 blocks) -> ReLU
    - There are skip connections between the encoder and decoder (as in the U-Net).

    Define the downsampler (encoder):
    """

    # down_model = downsample(3, 4)
    # down_result = down_model(tf.expand_dims(inp, 0))
    # print(down_result.shape)

    # up_model = upsample(3, 4)
    # up_result = up_model(down_result)
    # print(up_result.shape)

    """Visualize the generator model architecture:"""

    generator = Generator()
    # tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)

    """Test the generator:"""

    # gen_output = generator(inp[tf.newaxis, ...], training=use_training)

    # plt.imshow(gen_output[0, ...])

    """### Define the generator loss

    GANs learn a loss that adapts to the data, while cGANs learn a structured
    loss that penalizes a possible structure that differs from the network
    output and the target image, as described in the [pix2pix
    paper](https://arxiv.org/abs/1611.07004).

    - The generator loss is a sigmoid cross-entropy loss of the generated images
      and an **array of ones**.
    - The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute
      error) between the generated image and the target image.
    - This allows the generated image to become structurally similar to the
      target image.
    - The formula to calculate the total generator loss is `gan_loss + LAMBDA *
      l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of
      the paper.
    """

    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

    """The training procedure for the generator is as follows:

    ![Generator Update
    Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)
    """

    """Visualize the discriminator model architecture:"""

    discriminator = Discriminator()
    # tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)

    """Test the discriminator:"""

    # disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=use_training)
    # plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')
    # plt.colorbar()

    """The training procedure for the discriminator is shown below.

    To learn more about the architecture and the hyperparameters you can refer to
    the [pix2pix paper](https://arxiv.org/abs/1611.07004).

    ![Discriminator Update
    Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)

    ## Define the optimizers and a checkpoint-saver
    """

    generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=BETA_1)
    discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=BETA_1)

    make_checkpoint(CHECKPOINT_DIR, generator_optimizer, discriminator_optimizer, generator, discriminator)

    # To export the model in the latest checkpoint for converting to
    # tensorflow.js, set the below to True. Otherwise just loads latest
    # checkpoint.
    export_model(EXPORT_MODEL, generator, CHECKPOINT_DIR)

    """Test the function:"""

    # for example_input, example_target in test_dataset.take(1):
    #     generate_images(generator, example_input, example_target, use_training)

    """## Training

    - For each example input generates an output.
    - The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.
    - Next, calculate the generator and the discriminator loss.
    - Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.
    - Finally, log the losses to TensorBoard.
    """

    log_dir = "logs/"

    make_summary_writer(log_dir)

    @tf.function
    def train_step(input_image, target, step):
        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
            gen_output = generator(input_image, training=use_training)

            disc_real_output = discriminator([input_image, target], training=use_training)
            disc_generated_output = discriminator(
                [input_image, gen_output], training=use_training)

            gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(
                disc_generated_output, gen_output, target, loss_object
            )
            disc_loss = discriminator_loss(disc_real_output, disc_generated_output, loss_object)

        generator_gradients = gen_tape.gradient(gen_total_loss,
                                                generator.trainable_variables)
        discriminator_gradients = disc_tape.gradient(disc_loss,
                                                    discriminator.trainable_variables)

        generator_optimizer.apply_gradients(zip(generator_gradients,
                                                generator.trainable_variables))
        discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                                    discriminator.trainable_variables))

        write_to_summary(step, gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss)

        return gen_total_loss, disc_loss


    """The actual training loop. Since this tutorial can run of more than one
    dataset, and the datasets vary greatly in size the training loop is setup to
    work in steps instead of epochs.

    - Iterates over the number of steps.
    - Every 10 steps print a dot (`.`).
    - Every 1k steps: clear the display and run `generate_images` to show the progress.
    - Every 5k steps: save a checkpoint.
    """

    def fit(train_ds, test_ds, steps):
        example_input, example_target = next(iter(test_ds.take(1)))
        start = time.time()

        for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():
            if (step) % 1000 == 0:
                # display.clear_output(wait=True)

                if step != 0:
                    secs = time.time()-start
                    print(f'Time taken for 1000 steps: {secs:.2f} sec\n')
                    wandb.log({"secs_per_1000_steps": secs})

                # wandb.log({"checkpoint_saved_every_5k": step})

                start = time.time()

                generate_images(generator, example_input, example_target, use_training)
                print(f"Step: {step//1000}k")

            gen_total_loss, disc_loss = train_step(input_image, target, step)

            if (step + 1) % 250 == 0:
                wandb.log({"gen_loss": gen_total_loss, "disc_loss": disc_loss})

            # Training step
            if (step + 1) % 10 == 0:
                print('.', end='', flush=True)

            # Save (checkpoint) the model every 5k steps
            if (step + 1) % 5000 == 0:
                wandb.log({"checkpoint_saved_every_5k": step})
                save_checkpoint()


    """This training loop saves logs that you can view in TensorBoard to monitor the training progress.

    If you work on a local machine, you would launch a separate TensorBoard process. When working in a notebook, launch the viewer before starting the training to monitor with TensorBoard.

    To launch the viewer paste the following into a code-cell:
    """

    # Commented out IPython magic to ensure Python compatibility.
    # %load_ext tensorboard
    # %tensorboard --logdir {log_dir}

    """Finally, run the training loop:"""

    fit(train_dataset, test_dataset, steps=40000)

    """If you want to share the TensorBoard results _publicly_, you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.

    Note: This requires a Google account.

    ```
    !tensorboard dev upload --logdir {log_dir}
    ```

    Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the "interrupt execution" option in your notebook tool.

    You can view the [results of a previous run](https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).

    TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.

    It can also included inline using an `<iframe>`:
    """

    # display.IFrame(
    #     src="https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw",
    #     width="100%",
    #     height="1000px")

    """Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:

    - Check that neither the generator nor the discriminator model has "won". If either the `gen_gan_loss` or the `disc_loss` gets very low, it's an indicator that this model is dominating the other, and you are not successfully training the combined model.
    - The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.
    - For the `disc_loss`, a value below `0.69` means the discriminator is doing better than random on the combined set of real and generated images.
    - For the `gen_gan_loss`, a value below `0.69` means the generator is doing better than random at fooling the discriminator.
    - As training progresses, the `gen_l1_loss` should go down.

    ## Restore the latest checkpoint and test the network
    """

    # Restoring the latest checkpoint in CHECKPOINT_DIR
    restore_checkpoint()

    """## Generate some images using the test set"""

    # Run the trained model on a few examples from the test set
    for inp, tar in test_dataset.take(5):
        generate_images(generator, inp, tar, use_training)

if __name__ == "__main__":
    main()
